<!DOCTYPE html>
<html lang="en">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="icon" href="data:;base64,iVBORw0KGgo=">
<title>Rocket or Tower</title>
<link rel="stylesheet" type="text/css" href="../style.css">
<article>
    <header>
        <a href="/"><img src="../bird.png"></a>

    </header>

    <h1>Rocket or Tower</h1>

    <span class="article-date">Aug 2018</span>

    <h2>Attempt One</h2>

    <p>Recently I started working on <a href="https://fast.ai">fast.ai</a>, this is my attempt at creating a network.
    <p>My training set contains 10 rockets and 10 towers

    <img src="tower.png" />

    <p>Using the the 3 line program from the fast.ai library for the precompiled model <a href="https://github.com/KaimingHe/deep-residual-networks">resnet</a>

    <pre><code>
  arch=resnet34
  data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
  learn = ConvLearner.pretrained(arch, data, precompute=True)
  learn.fit(0.01, 5)
  </code></pre>

    <p>0 is more rocket-like, 1 is more building-like</p>

    <p>With having only 10 examples of each. It is amazing how well it did. Only misclassifing the CN tower just barely to being a rocket

    <img src="result.png" width="722px" />

    <h2>Attempt Two</h2>

    <p>I added a lot more photos to about 300 each for training and 15 for the validation

    <p>This time adding data augmentation, variable learning rate and unfreezing the model

    <h3>Data Augmentation</h3>

    <p>Modify the image slightly to known effects of a camera. This is transformations that shift, zoom and flip the images. These are fed into the learning like any other image

    <pre><code>tfms = tfms_from_model(resnet34, sz, aug_tfms=transforms_side_on, max_zoom=1.1)</pre></code>

    <img src="aug.png" />


    <h3>Stochastic Gradient Descent with Restarts</h3>

    <p>When a network is learning a function. It needs to start off high and reduce to gain more accuracy. This is included but also is restarted every epoch. This helps generalize the network, by trying avoiding narrow minimums. Creating a learning rate like this...

    <img src="learning.png" />

    <p>This is improved a little bit more by slowing down the learning rate on the later epochs.

    <img src="learning2.png" />

    <h3>Training the precomputed Model</h3>

    <p>Before training was done on the last few layers. This opens the begining layers. Also much lower learning rates are used in decreasing order to the earlier layers

    <pre><code>  learn.unfreeze()
  lr=np.array([1e-4,1e-3,1e-2])
  learn.fit(lr, 3, cycle_len=1, cycle_mult=2)</code></pre>

    <h3>Result</h3>

    <p>96.6% accuracy. With one error in classification

    <pre><code>  plot_confusion_matrix(cm, data.classes)</code></pre>

    <img src="confusion.png" />

    <p>Below it the images it was most uncertain of including the incorrect labeling of the rocket on the bottom left

    <img src="result2.png" width="722px"/>

    <p>Thank you</p>


</article>
