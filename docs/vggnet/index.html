<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" href="favicon.ico" type="image/x-icon"/>
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon"/>

  <title>James Pucula - Visualizing VGGNet Activations</title>

  <style>@import url('https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap');</style>
  <script>
    (function() {
      try {
        var stored = localStorage.getItem('theme');
        var mediaQuery = window.matchMedia ? window.matchMedia('(prefers-color-scheme: dark)') : null;
        var prefersDark = mediaQuery ? mediaQuery.matches : false;
        var theme = stored || (prefersDark ? 'dark' : 'light');
        document.documentElement.dataset.theme = theme;
      } catch (err) {
        document.documentElement.dataset.theme = 'light';
      }
    })();
  </script>
  <link rel = "stylesheet" type="text/css" href="/style.css"/>

</head>
<body>
  <header>
        <a href="/">
  <img width="64" height="64" src="/finch_128.png"></a>
    <button id="theme-toggle" class="theme-toggle" type="button">Dark mode</button>
  </header>

  <main>
<h1>Visualizing VGGNet Activations</h1>

<p>
    While working through a neural network optimisation course I wanted to see
    what a pruned VGG-style model actually latches onto. This post is a log of
    the setup, the dataset, and the quirks I noticed while poking at the
    activations.
</p>

<h2>Model setup</h2>
<p>
    The backbone is a VGG19-inspired network trimmed to accept 32x32 inputs. It
    clocks in at 9.23 million parameters and about 606 million MACs before any
    pruning. I ran granular pruning to zero out weights below a learned
    threshold, which cut the parameter count nearly in half without breaking the
    output distribution. Because the pruning mostly deletes values that were
    already close to zero, the activation patterns in the remaining filters stay
    surprisingly intact.
</p>

<p>
    Hooks were registered on every convolutional block immediately after the
    ReLU. During a forward pass the tensors are copied to CPU, normalised per
    channel, and saved as PNG grids. Nothing fancy--just PyTorch hooks feeding a
    small Matplotlib helper.
</p>

<h2>Dataset</h2>
<p>
    All experiments were run on
    <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>. The test
    split became the playground for visualisation so everything shown here was
    unseen at training time. The ten classes (airplane through truck) are compact
    enough to keep runs quick but still diverse enough to make the filters learn
    interesting colour and texture detectors.
</p>

<h2>What the activations show</h2>
<p>
    The example below uses a frog image from the test set. Darker green indicates
    stronger positive activation after the ReLU.
    <br />
    <img src="output/0-4.png" height="100px" />
</p>

<p>
    The first two convolutional blocks respond to broad colour blobs and the
    curved outline of the body. Mid-level blocks start to separate the frog from
    the background water ripple thanks to the narrow receptive fields focusing on
    texture. By the time the signal hits the deeper layers the activations look
    nothing like the original image--mostly sparse highlights where the network is
    convinced it has found amphibian skin. The classifier head still hedges a bit
    (frog vs deer), but the high-confidence channels correspond to filters that
    survived pruning, which was a nice sanity check.
</p>

<p>
    The grid below captures every recorded activation map for that sample. It is
    dense, but scrolling through it made it clear which filters are dead after
    pruning and which ones still carry the classification.
</p>

<img src="output/Screenshot from 2025-03-29 10-34-15.png" alt="Activation grids for VGG-style model" />
  </main>
  <script>
    (function() {
      var toggle = document.getElementById('theme-toggle');
      if (!toggle) {
        return;
      }

      var root = document.documentElement;

      function currentTheme() {
        return root.dataset.theme === 'dark' ? 'dark' : 'light';
      }

      function updateLabel(theme) {
        var next = theme === 'dark' ? 'light' : 'dark';
        toggle.textContent = next.charAt(0).toUpperCase() + next.slice(1) + ' mode';
        toggle.setAttribute('aria-label', 'Activate ' + next + ' mode');
        toggle.setAttribute('aria-pressed', theme === 'dark' ? 'true' : 'false');
      }

      updateLabel(currentTheme());

      toggle.addEventListener('click', function() {
        var next = currentTheme() === 'dark' ? 'light' : 'dark';
        root.dataset.theme = next;
        try {
          localStorage.setItem('theme', next);
        } catch (err) {
          /* localStorage might be unavailable; ignore */
        }
        updateLabel(next);
      });
    })();
  </script>
  </body>
</html>
